import { BaseContext } from '../../lib/component/baseContext';
import { MessagePayload } from '../../lib/message';
import { PostbackAction } from '../messagev2';
import { ChatEntry } from './llmComponentTypes';
/**
 * The Bots LlmComponentContext is a class with convenience methods when working wih large language models
 * </p>
 * An LlmComponentContext class instance is passed as an argument to every llm event handler function.
 * @memberof module:Lib
 * @extends BaseContext
 * @alias LlmComponentContext
 */
export declare class LlmComponentContext extends BaseContext {
    private _chatEntries;
    private _turn;
    /**
     * Constructor of rest service context.
     * DO NOT USE - INSTANCE IS ALREADY PASSED TO EVENT HANDLERS
     * @param {object} request
     */
    constructor(request: any);
    /**
     * Adds a message to the bot response sent to the user.
     * @param {object} payload - can take a string message, or a message created using the MessageFactory
     */
    addMessage(payload: string | MessagePayload): void;
    /**
     * Set a transition action. When you use this function, the dialog engine will transition to the state defined for this transition action.
     * <p>
     * @param {string} action - name of the transition action
     */
    setTransitionAction(action: string): void;
    /**
     * Sets an LLM prompt that will be sent next to the LLM
     * <p>
     * @param {string} prompt - the text of the prompt
     * @param {boolean} isRetry - is the prompt used to try to fix a prior invalid LLM response
     */
    setNextLLMPrompt(prompt: string, isRetry: boolean): void;
    /**
     * Set the value of the LLM result variable
     * <p>
     * @param {object} result - the value
     */
    setResultVariable(result: object): void;
    /**
     * Get the value of the LLM result variable
     * <p>
     * @returns {object} the result value
     */
    getResultVariable(): object | undefined;
    /**
     * Array of chat messages that are exchanged with the LLM.
     * Each message has the following properties:
     * - role: the role under which the message is sent: system, user or assistant
     * - content: the message text
     * - turn: number indicating the refinement turn of the chat messages exchange
     * @returns {ChatEntry[]} the chat messages
     */
    getChatHistory(): ChatEntry[];
    /**
     * Returns number indicating the current refinement turn of the chat messages exchange.
     * When the first prompt is sent to the LLM the turn is 1.
     * @returns {number} the turn
     */
    getCurrentTurn(): number;
    /**
     * Returns the LLM system prompt
     * @returns {string} the prompt
     */
    getSystemPrompt(): string;
    /**
     * Returns the last response sent by the LLM
     * @returns {ChatEntry} the message
     */
    getLastAssistantMessage(): ChatEntry;
    /**
     * Returns the last message sent by the user
     * @returns {ChatEntry} the message
     */
    getLastUserMessage(): ChatEntry;
    /**
     * Update the LLM system prompt
     * @param {string} prompt - the new prompt
     */
    updateSystemPrompt(prompt: string): void;
    /**
     * Update the last LLM response message
     * @param {string} message - the new message
     */
    updateLastAssistantMessage(message: string): void;
    /**
     * Set the request or LLM response validation error
     * @param {string} errorMessage - the error message
     * @param {string} errorCode - allowable values: 'requestFlagged', 'responseFlagged', 'requestInvalid',
     *  'responseInvalid', 'modelLengthExceeded'
     */
    setValidationError(errorMessage: string, errorCode?: 'requestFlagged' | 'responseFlagged' | 'requestInvalid' | 'responseInvalid' | 'modelLengthExceeded'): void;
    /**
     * Converts the message to a JSON object:
     * - it first search the first occurrence of open-curly-bracket '{' and last occurrence of close-curly-bracket '}'
     * - it then tries to parse the message between the open and close curly brackets into a JSON object.
     * - if parsing is successful, the JSON object is returned, otherwise the method returns undefined
     * @param {string} message - the message to convert
     * @returns {object | undefined} the parsed message, or undefined
     */
    convertToJSON(message: string): object | undefined;
    /**
     * Returns true when JSON formatting is enabled in the associated Invoke LLM state in Visual Fow Designer
     */
    isJsonValidationEnabled(): boolean;
    /**
     * Returns the number of retry prompts that have been sent to the LLM since the last successful LLM response
     */
    getRetries(): number;
    /**
     * Returns the maximum number of retry prompts that will be sent to the LLM when the response is invalid
     * @returns {number} the maximum number
     */
    getMaxRetries(): number;
    /**
     * Returns the status message that is sent to the user when the LLM is invoked with a retry prompt
     * @returns {string} the message
     */
    getRetryUserMessage(): string | undefined;
    /**
     * Returns the JSON schema used to validate the LLM response
     * @returns {object} the json schema
     */
    getJsonSchema(): object | undefined;
    /**
     * Returns the template used to enrich the system prompt with instructions to comply with a JSON schema
     * @returns {string} the instrution
     */
    getJsonSchemaInstructionTemplate(): string;
    /**
     * Returns the template used to send a retry prompt to the LLM when validation errors have been found.
     * @returns {string} the instrution
     */
    getInvalidResponseTemplate(): string;
    /**
     * Sets the value of a custom property that is stored in the LLM context. A custom property can be
     * used to maintain custom state accross event handler calls while interacting with the LLM within the
     * current state in visual flow designer.
     * If you set the value to null, the custom property will be removed.
     * @param {string} name - name of the custom property
     * @param {object} value - value of the custom property
     */
    setCustomProperty(name: string, value: any): void;
    /**
     * Returns the value of a custom property that is stored in the LLM context. A custom property can be
     * used to maintain custom state accross event handler calls while interacting with the LLM within the
     * current state  in visual flow designer.
     * @return {object} value of the custom property
     * @param {string} name - name of the custom property
     */
    getCustomProperty(name: string): any;
    /**
     * Create a postback action that sends a new prompt to the LLM.
     * <p>
     * @param {string} label - the label of the postback action button
     * @param {string} prompt - the text of the prompt
     * @returns {PostbackAction} the postback action
     */
    createLLMPromptAction(label: string, prompt: string): PostbackAction;
    /**
     * Enriches the system prompt with JSON schema formatting instruction
     */
    addJSONSchemaFormattingInstruction(): void;
    /**
     * Handles an invalid LLM response by sending a retry prompt to the LLM if the maximum number of retries has
     * not been reached yet. If maximum number of retries is reached, a validation error is set which results in a
     * transition out of the LLM component using the 'error' transition.
     * <p>
     * @param {string[]} errors - messages describing what is invalid about the response
     * @returns {false} always returns false
     */
    handleInvalidResponse(errors: string[]): false;
    private getLastMessage;
}
